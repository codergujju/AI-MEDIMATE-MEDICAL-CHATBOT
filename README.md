# ğŸ¤– AI-MEDIMATE-MEDICAL-CHATBOT

AI MEDIMATE is an intelligent medical chatbot built using Streamlit, NLP, Semantic Search, and Large Language Models (LLMs).
It provides accurate and user-friendly answers to general medical queries by combining verified NIH medical data with LLM-enhanced responses using a RAG (Retrieval-Augmented Generation) approach.

ğŸ“Œ Features

ğŸ¦  Ask questions about diseases (diabetes, cancer, asthma, etc.)

âš  Get details on symptoms (fever, cough, chest pain, weakness)

ğŸ” Understand diagnosis & medical conditions

ğŸ’Š Learn about treatments, medicines & side effects

ğŸ§ª Information on medical tests (CBC, MRI, X-ray, etc.)

ğŸ“š Uses 47,000+ verified medical Q&A pairs from NIH MedQuAD

ğŸ§  Hybrid AI: Semantic Search + LLM

ğŸ’¬ ChatGPT-style conversational UI

ğŸ—‘ï¸ Clear Chat option

âš¡ Fast, simple, and interactive interface

ğŸ› ï¸ Tech Stack

Python

Streamlit â€“ Web UI

Sentence Transformers â€“ Text embeddings

Scikit-learn â€“ Cosine similarity

Ollama â€“ LLM inference

Pandas â€“ Dataset handling

ğŸ“‚ Project Structure
AI-MEDIMATE-MEDICAL-CHATBOT/
â”‚
â”œâ”€â”€ App.py                 # Streamlit frontend
â”œâ”€â”€ model4.py              # Hybrid RAG + LLM backend
â”œâ”€â”€ dataset_QA.csv         # NIH MedQuAD medical dataset
â”œâ”€â”€ Medichatbot_demo.mp4   # Demo video
â”œâ”€â”€ README.md              # Documentation

âš™ï¸ How the System Works (RAG Pipeline)

User enters a medical query

Query is converted into embeddings using SentenceTransformer

Similar medical Q&A pairs are retrieved using cosine similarity

Retrieved answer is passed to an LLM via Ollama

Chatbot displays:

ğŸ§  LLM Answer (refined & conversational)

ğŸ“š Database Answer (original dataset response)

ğŸ§  LLM Models Used (via Ollama)

AI MEDIMATE supports multiple local LLMs using Ollama.
ğŸ“Š Dataset

This project uses the NIH MedQuAD (Medical Question Answering Dataset), a trusted and publicly available medical dataset.

ğŸ”— Dataset Link: ğŸ‘‰ https://github.com/abachaa/MedQuAD

Dataset Highlights

47,000+ medical questionâ€“answer pairs

Curated from NIH-trusted medical sources

Covers diseases, drugs, diagnosis, treatments & tests

ğŸ”¹ Supported Models
Model Name	Ollama Identifier	Description
LLaMA 3.2 (13B)	llama3.2:latest	High-quality medical explanations
Mistral 7B	mistral:latest	Fast and efficient responses
Falcon 7B	falcon:latest	Lightweight and reliable

You can switch models easily inside model4.py.

ğŸ’» LLM System Requirements
ğŸ”¹ Minimum Requirements

OS: Windows / Linux / macOS

RAM: 8 GB (for Mistral / Falcon)

CPU: Modern multi-core CPU

Storage: 10â€“15 GB free space

ğŸ”¹ Recommended (Best Performance)

RAM: 16 GB or more

GPU: Optional (NVIDIA GPU improves speed)

Storage: 20+ GB free

âš  LLaMA 13B models require higher RAM and run slower on low-end systems.

ğŸ”§ Ollama Installation & Setup
1ï¸âƒ£ Install Ollama

ğŸ‘‰ https://ollama.com

2ï¸âƒ£ Pull a model
ollama pull llama3.2


(or)

ollama pull mistral

3ï¸âƒ£ Verify installation
ollama list

ğŸš€ Run the Application
Install Python dependencies
pip install streamlit pandas sentence-transformers scikit-learn ollama

Start the chatbot
streamlit run App.py

ğŸ¥ Demo

ğŸ“¹ Refer to Medichatbot_demo.mp4 for a full demonstration of the chatbot workflow and UI.

âš ï¸ Disclaimer

This chatbot is intended for educational and informational purposes only.
It does not replace professional medical advice, diagnosis, or treatment.

ğŸ‘¨â€ğŸ’» Developer

Vishesh Chavda
Data Scientist | Machine Learning Enthusiast
